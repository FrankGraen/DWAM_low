seed: 42

# Models are instantiated using skrl's model instantiator utility
# https://skrl.readthedocs.io/en/develop/modules/skrl.utils.model_instantiators.html
models:
  separate: False #策略网络（policy）和价值网络（value）是否分开训练
  policy:  # see skrl.utils.model_instantiators.gaussian_model for parameter details
    clip_actions: True #是否裁剪动作(限制动作范围)
    clip_log_std: True
    min_log_std: -20.0
    max_log_std: 2.0
    input_shape: "Shape.STATES"
    hiddens: [128, 128]
    hidden_activation: ["leaky_relu", "leaky_relu", "leaky_relu"]
    output_shape: "Shape.ACTIONS"
    output_activation: "tanh"
    output_scale: 1.0
  value:  # see skrl.utils.model_instantiators.deterministic_model for parameter details
    clip_actions: False
    input_shape: "Shape.STATES"
    hiddens: [128, 128]
    hidden_activation: ["leaky_relu", "leaky_relu", "leaky_relu"]
    output_shape: "Shape.ONE"
    output_activation: ""
    output_scale: 1.0


#   learning_rate_scheduler: "KLAdaptiveRL"
agent:
  rollouts: 32
  learning_epochs: 8
  mini_batches: 32
  discount_factor: 0.99
  lambda: 0.95
  learning_rate: 2.5e-4

  #learning_rate_scheduler: "None"
  #learning_rate_scheduler_kwargs: null
  #state_preprocessor: "None"
  #state_preprocessor_kwargs: null
  #value_preprocessor: "None"
  #value_preprocessor_kwargs: null
  random_timesteps: 0
  learning_starts: 0
  grad_norm_clip: 0.5 #1.0
  ratio_clip: 0.2
  value_clip: 0.2
  clip_predicted_values: True
  entropy_loss_scale: 0.0
  value_loss_scale: 1.0
  kl_threshold: 0.008
  rewards_shaper_scale: 0.01
  rewards_shaper: "None"
  # logging and checkpoint
  experiment:
    directory: "PushBox"
    experiment_name: ""
    write_interval: 40 #
    checkpoint_interval: 400 #
    wandb: True            # whether to use Weights & Biases

trainer:
  timesteps: 1000000 # 100万步


# 全局参数
# seed: 42
# 用途: 设置随机种子，用于确保实验的可重复性。固定种子可以使随机过程（如权重初始化、数据采样）产生一致的结果。
# 模型配置（models）
# separate: False
# 用途: 表示策略网络（policy）和价值网络（value）是否分开训练。False表示两者共享部分参数（通常是特征提取层）。
# policy
# 用途: 定义策略网络（输出动作分布）的结构和行为，用于决定智能体在给定状态下采取的动作。
# 参数解释:
# clip_actions: True: 是否将输出的动作限制在合法范围内（例如[-1, 1]）。
# clip_log_std: True: 是否限制动作分布的对数标准差（log standard deviation），以控制探索的稳定性。
# min_log_std: -20.0: 对数标准差的最小值，避免分布过于集中。
# max_log_std: 2.0: 对数标准差的最大值，避免分布过于分散。
# input_shape: "Shape.STATES": 输入形状，通常是环境的观测空间维度。
# hiddens: [32, 32]: 隐藏层的神经元数量，定义网络深度和宽度（这里是两层，每层32个神经元）。
# hidden_activation: ["leaky_relu", "leaky_relu", "leaky_relu"]: 隐藏层的激活函数，使用Leaky ReLU以避免梯度消失。
# output_shape: "Shape.ACTIONS": 输出形状，通常是动作空间的维度。
# output_activation: "tanh": 输出层的激活函数，使用tanh将动作缩放到[-1, 1]。
# output_scale: 1.0: 输出缩放因子，用于调整动作范围。
# value
# 用途: 定义价值网络，估计状态的价值（通常是期望累积回报），用于指导策略优化。
# 参数解释:
# clip_actions: False: 价值网络不涉及动作输出，因此不需要动作裁剪。
# input_shape: "Shape.STATES": 输入形状，与策略网络相同。
# hiddens: [32, 32]: 隐藏层结构，与策略网络一致。
# hidden_activation: ["leaky_relu", "leaky_relu", "leaky_relu"]: 激活函数，与策略网络一致。
# output_shape: "Shape.ONE": 输出单一值，表示状态的价值。
# output_activation: "": 输出层无激活函数，直接输出价值。
# output_scale: 1.0: 输出缩放因子，通常保持为1。
# 代理配置（agent）
# rollouts: 60
# 用途: 每次策略更新的采样回合数，表示收集多少条交互数据（状态-动作-奖励序列）。
# learning_epochs: 4
# 用途: 每次更新时，对采样数据进行多少次完整的训练迭代（epoch）。
# mini_batches: 60
# 用途: 将采样数据分成多少个小批量（mini-batch）进行梯度更新，控制每次更新的数据量。
# discount_factor: 0.99
# 用途: 折扣因子（γ），用于计算未来奖励的现值，值越接近1，智能体越重视长期回报。
# lambda: 0.95
# 用途: GAE（Generalized Advantage Estimation）的λ参数，平衡偏差和方差，用于计算优势函数。
# learning_rate: 1.e-4
# 用途: 学习率，控制网络参数更新的步长，较小的值（如0.0001）使训练更稳定。
# grad_norm_clip: 0.5
# 用途: 梯度裁剪的最大范数，防止梯度爆炸，稳定训练。
# ratio_clip: 0.2
# 用途: PPO算法的策略比率裁剪范围，限制新旧策略之间的差异，保持更新稳定性。
# value_clip: 0.2
# 用途: 价值函数的裁剪范围，限制价值预测的变化，增强训练稳定性。
# clip_predicted_values: True
# 用途: 是否对预测的价值进行裁剪，与value_clip配合使用。
# entropy_loss_scale: 0.0
# 用途: 熵损失的权重，鼓励策略的探索性。值为0表示不使用熵正则化。
# value_loss_scale: 1.0
# 用途: 价值损失的权重，控制价值函数优化的重要性。
# kl_threshold: 0.008
# 用途: KL散度阈值，用于自适应调整学习率或策略更新，防止策略变化过大。
# experiment
# directory: "PushBox": 实验结果保存的目录。
# experiment_name: "": 实验名称，留空可能使用默认命名。
# write_interval: 40: 每40次迭代记录一次日志（如损失、奖励）。
# checkpoint_interval: 400: 每400次迭代保存一次模型检查点。
# wandb: True: 是否使用Weights & Biases进行实验跟踪和可视化。
# 训练器配置（trainer）
# timesteps: 1000000
# 用途: 训练的总时间步数，表示智能体与环境交互的总步数（100万步）。

